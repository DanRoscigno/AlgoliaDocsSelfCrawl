"use strict";(self.webpackChunkclickhouse_docs_2_3_0=self.webpackChunkclickhouse_docs_2_3_0||[]).push([[93322],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>c});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=n.createContext({}),s=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=s(e.components);return n.createElement(p.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},k=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,p=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),u=s(a),k=r,c=u["".concat(p,".").concat(k)]||u[k]||m[k]||l;return a?n.createElement(c,i(i({ref:t},d),{},{components:a})):n.createElement(c,i({ref:t},d))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=k;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o[u]="string"==typeof e?e:r,i[1]=o;for(var s=2;s<l;s++)i[s]=a[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}k.displayName="MDXCreateElement"},46055:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>m,frontMatter:()=>l,metadata:()=>o,toc:()=>s});var n=a(87462),r=(a(67294),a(3905));const l={slug:"/en/engines/table-engines/integrations/hdfs",sidebar_position:6,sidebar_label:"HDFS"},i="HDFS",o={unversionedId:"en/engines/table-engines/integrations/hdfs",id:"en/engines/table-engines/integrations/hdfs",title:"HDFS",description:"This engine provides integration with the Apache Hadoop ecosystem by allowing to manage data on HDFS via ClickHouse. This engine is similar to the File and URL engines, but provides Hadoop-specific features.",source:"@site/docs/en/engines/table-engines/integrations/hdfs.md",sourceDirName:"en/engines/table-engines/integrations",slug:"/en/engines/table-engines/integrations/hdfs",permalink:"/AlgoliaDocsSelfCrawl/en/engines/table-engines/integrations/hdfs",draft:!1,editUrl:"https://github.com/ClickHouse/ClickHouse/tree/master/docs/en/engines/table-engines/integrations/hdfs.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{slug:"/en/engines/table-engines/integrations/hdfs",sidebar_position:6,sidebar_label:"HDFS"},sidebar:"english",previous:{title:"MongoDB",permalink:"/AlgoliaDocsSelfCrawl/en/engines/table-engines/integrations/mongodb"},next:{title:"S3",permalink:"/AlgoliaDocsSelfCrawl/en/engines/table-engines/integrations/s3"}},p={},s=[{value:"Usage",id:"usage",level:2},{value:"PARTITION BY",id:"partition-by",level:3},{value:"Implementation Details",id:"implementation-details",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Configuration Options",id:"configuration-options",level:3},{value:"Supported by libhdfs3",id:"supported-by-libhdfs3",level:4},{value:"ClickHouse extras",id:"clickhouse-extras",level:4},{value:"Limitations",id:"limitations",level:3},{value:"Kerberos support",id:"kerberos-support",level:2},{value:"HDFS Namenode HA support",id:"namenode-ha",level:2},{value:"Virtual Columns",id:"virtual-columns",level:2}],d={toc:s},u="wrapper";function m(e){let{components:t,...a}=e;return(0,r.kt)(u,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"hdfs"},"HDFS"),(0,r.kt)("p",null,"This engine provides integration with the ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Apache_Hadoop"},"Apache Hadoop")," ecosystem by allowing to manage data on ",(0,r.kt)("a",{parentName:"p",href:"https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html"},"HDFS")," via ClickHouse. This engine is similar to the ",(0,r.kt)("a",{parentName:"p",href:"/AlgoliaDocsSelfCrawl/en/engines/table-engines/special/file#table_engines-file"},"File")," and ",(0,r.kt)("a",{parentName:"p",href:"/AlgoliaDocsSelfCrawl/en/engines/table-engines/special/url#table_engines-url"},"URL")," engines, but provides Hadoop-specific features."),(0,r.kt)("h2",{id:"usage"},"Usage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"ENGINE = HDFS(URI, format)\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Engine Parameters")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"URI")," - whole file URI in HDFS. The path part of ",(0,r.kt)("inlineCode",{parentName:"li"},"URI")," may contain globs. In this case the table would be readonly."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"format")," - specifies one of the available file formats. To perform\n",(0,r.kt)("inlineCode",{parentName:"li"},"SELECT")," queries, the format must be supported for input, and to perform\n",(0,r.kt)("inlineCode",{parentName:"li"},"INSERT")," queries \u2013 for output. The available formats are listed in the\n",(0,r.kt)("a",{parentName:"li",href:"/AlgoliaDocsSelfCrawl/en/interfaces/formats#formats"},"Formats")," section."),(0,r.kt)("li",{parentName:"ul"},"[PARTITION BY expr]")),(0,r.kt)("h3",{id:"partition-by"},"PARTITION BY"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"PARTITION BY")," \u2014 Optional. In most cases you don't need a partition key, and if it is needed you generally don't need a partition key more granular than by month. Partitioning does not speed up queries (in contrast to the ORDER BY expression). You should never use too granular partitioning. Don't partition your data by client identifiers or names (instead, make client identifier or name the first column in the ORDER BY expression)."),(0,r.kt)("p",null,"For partitioning by month, use the ",(0,r.kt)("inlineCode",{parentName:"p"},"toYYYYMM(date_column)")," expression, where ",(0,r.kt)("inlineCode",{parentName:"p"},"date_column")," is a column with a date of the type ",(0,r.kt)("a",{parentName:"p",href:"/AlgoliaDocsSelfCrawl/en/sql-reference/data-types/date"},"Date"),". The partition names here have the ",(0,r.kt)("inlineCode",{parentName:"p"},'"YYYYMM"')," format."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example:")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"1.")," Set up the ",(0,r.kt)("inlineCode",{parentName:"p"},"hdfs_engine_table")," table:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE hdfs_engine_table (name String, value UInt32) ENGINE=HDFS('hdfs://hdfs1:9000/other_storage', 'TSV')\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"2.")," Fill file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO hdfs_engine_table VALUES ('one', 1), ('two', 2), ('three', 3)\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"3.")," Query the data:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM hdfs_engine_table LIMIT 2\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"\u250c\u2500name\u2500\u252c\u2500value\u2500\u2510\n\u2502 one  \u2502     1 \u2502\n\u2502 two  \u2502     2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,r.kt)("h2",{id:"implementation-details"},"Implementation Details"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Reads and writes can be parallel.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Not supported:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ALTER")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"SELECT...SAMPLE")," operations."),(0,r.kt)("li",{parentName:"ul"},"Indexes."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/AlgoliaDocsSelfCrawl/en/operations/storing-data#zero-copy"},"Zero-copy")," replication is possible, but not recommended.")),(0,r.kt)("admonition",{parentName:"li",title:"Zero-copy replication is not ready for production",type:"warning"},(0,r.kt)("p",{parentName:"admonition"},"Zero-copy replication is disabled by default in ClickHouse version 22.8 and higher.  This feature is not recommended for production use.")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Globs in path")),(0,r.kt)("p",null,"Multiple path components can have globs. For being processed file should exists and matches to the whole path pattern. Listing of files determines during ",(0,r.kt)("inlineCode",{parentName:"p"},"SELECT")," (not at ",(0,r.kt)("inlineCode",{parentName:"p"},"CREATE")," moment)."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"*")," \u2014 Substitutes any number of any characters except ",(0,r.kt)("inlineCode",{parentName:"li"},"/")," including empty string."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"?")," \u2014 Substitutes any single character."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"{some_string,another_string,yet_another_one}")," \u2014 Substitutes any of strings ",(0,r.kt)("inlineCode",{parentName:"li"},"'some_string', 'another_string', 'yet_another_one'"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"{N..M}")," \u2014 Substitutes any number in range from N to M including both borders.")),(0,r.kt)("p",null,"Constructions with ",(0,r.kt)("inlineCode",{parentName:"p"},"{}")," are similar to the ",(0,r.kt)("a",{parentName:"p",href:"/AlgoliaDocsSelfCrawl/en/sql-reference/table-functions/remote"},"remote")," table function."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Suppose we have several files in TSV format with the following URIs on HDFS:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"'hdfs://hdfs1:9000/some_dir/some_file_1'"),(0,r.kt)("li",{parentName:"ul"},"'hdfs://hdfs1:9000/some_dir/some_file_2'"),(0,r.kt)("li",{parentName:"ul"},"'hdfs://hdfs1:9000/some_dir/some_file_3'"),(0,r.kt)("li",{parentName:"ul"},"'hdfs://hdfs1:9000/another_dir/some_file_1'"),(0,r.kt)("li",{parentName:"ul"},"'hdfs://hdfs1:9000/another_dir/some_file_2'"),(0,r.kt)("li",{parentName:"ul"},"'hdfs://hdfs1:9000/another_dir/some_file_3'"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"There are several ways to make a table consisting of all six files:"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE table_with_range (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV')\n")),(0,r.kt)("p",null,"Another way:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE table_with_question_mark (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_?', 'TSV')\n")),(0,r.kt)("p",null,"Table consists of all the files in both directories (all files should satisfy format and schema described in query):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE table_with_asterisk (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV')\n")),(0,r.kt)("admonition",{type:"warning"},(0,r.kt)("p",{parentName:"admonition"},"If the listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ",(0,r.kt)("inlineCode",{parentName:"p"},"?"),".")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example")),(0,r.kt)("p",null,"Create table with files named ",(0,r.kt)("inlineCode",{parentName:"p"},"file000"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"file001"),", \u2026 , ",(0,r.kt)("inlineCode",{parentName:"p"},"file999"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE big_table (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV')\n")),(0,r.kt)("h2",{id:"configuration"},"Configuration"),(0,r.kt)("p",null,"Similar to GraphiteMergeTree, the HDFS engine supports extended configuration using the ClickHouse config file. There are two configuration keys that you can use: global (",(0,r.kt)("inlineCode",{parentName:"p"},"hdfs"),") and user-level (",(0,r.kt)("inlineCode",{parentName:"p"},"hdfs_*"),"). The global configuration is applied first, and then the user-level configuration is applied (if it exists)."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'  \x3c!-- Global configuration options for HDFS engine type --\x3e\n  <hdfs>\n    <hadoop_kerberos_keytab>/tmp/keytab/clickhouse.keytab</hadoop_kerberos_keytab>\n    <hadoop_kerberos_principal>clickuser@TEST.CLICKHOUSE.TECH</hadoop_kerberos_principal>\n    <hadoop_security_authentication>kerberos</hadoop_security_authentication>\n  </hdfs>\n\n  \x3c!-- Configuration specific for user "root" --\x3e\n  <hdfs_root>\n    <hadoop_kerberos_principal>root@TEST.CLICKHOUSE.TECH</hadoop_kerberos_principal>\n  </hdfs_root>\n')),(0,r.kt)("h3",{id:"configuration-options"},"Configuration Options"),(0,r.kt)("h4",{id:"supported-by-libhdfs3"},"Supported by libhdfs3"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"parameter")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default value")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","connect","_","tcpnodelay"),(0,r.kt)("td",{parentName:"tr",align:null},"true")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","client","_","read","_","shortcircuit"),(0,r.kt)("td",{parentName:"tr",align:null},"true")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","replace-datanode-on-failure"),(0,r.kt)("td",{parentName:"tr",align:null},"true")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","notretry-another-node"),(0,r.kt)("td",{parentName:"tr",align:null},"false")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","localread","_","mappedfile"),(0,r.kt)("td",{parentName:"tr",align:null},"true")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","client","_","use","_","legacy","_","blockreader","_","local"),(0,r.kt)("td",{parentName:"tr",align:null},"false")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","ping","_","interval"),(0,r.kt)("td",{parentName:"tr",align:null},"10  * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","connect","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","read","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","write","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","socekt","_","linger","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"-1")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","connect","_","retry"),(0,r.kt)("td",{parentName:"tr",align:null},"10")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rpc","_","client","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","default","_","replica"),(0,r.kt)("td",{parentName:"tr",align:null},"3")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","connect","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","read","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","write","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","localread","_","default","_","buffersize"),(0,r.kt)("td",{parentName:"tr",align:null},"1 ",(0,r.kt)("em",{parentName:"td"}," 1024 ")," 1024")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","prefetchsize"),(0,r.kt)("td",{parentName:"tr",align:null},"10")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","read","_","getblockinfo","_","retry"),(0,r.kt)("td",{parentName:"tr",align:null},"3")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","localread","_","blockinfo","_","cachesize"),(0,r.kt)("td",{parentName:"tr",align:null},"1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"input","_","read","_","max","_","retry"),(0,r.kt)("td",{parentName:"tr",align:null},"60")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","default","_","chunksize"),(0,r.kt)("td",{parentName:"tr",align:null},"512")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","default","_","packetsize"),(0,r.kt)("td",{parentName:"tr",align:null},"64 * 1024")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","default","_","write","_","retry"),(0,r.kt)("td",{parentName:"tr",align:null},"10")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","connect","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","read","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","write","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","close","_","timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"3600 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","packetpool","_","size"),(0,r.kt)("td",{parentName:"tr",align:null},"1024")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"output","_","heeartbeat","_","interval"),(0,r.kt)("td",{parentName:"tr",align:null},"10 * 1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","client","_","failover","_","max","_","attempts"),(0,r.kt)("td",{parentName:"tr",align:null},"15")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","client","_","read","_","shortcircuit","_","streams","_","cache","_","size"),(0,r.kt)("td",{parentName:"tr",align:null},"256")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","client","_","socketcache","_","expiryMsec"),(0,r.kt)("td",{parentName:"tr",align:null},"3000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","client","_","socketcache","_","capacity"),(0,r.kt)("td",{parentName:"tr",align:null},"16")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","default","_","blocksize"),(0,r.kt)("td",{parentName:"tr",align:null},"64 ",(0,r.kt)("em",{parentName:"td"}," 1024 ")," 1024")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","default","_","uri"),(0,r.kt)("td",{parentName:"tr",align:null},'"hdfs://localhost:9000"')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hadoop","_","security","_","authentication"),(0,r.kt)("td",{parentName:"tr",align:null},'"simple"')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hadoop","_","security","_","kerberos","_","ticket","_","cache","_","path"),(0,r.kt)("td",{parentName:"tr",align:null},'""')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","client","_","log","_","severity"),(0,r.kt)("td",{parentName:"tr",align:null},'"INFO"')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"dfs","_","domain","_","socket","_","path"),(0,r.kt)("td",{parentName:"tr",align:null},'""')))),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://hawq.apache.org/docs/userguide/2.3.0.0-incubating/reference/HDFSConfigurationParameterReference.html"},"HDFS Configuration Reference")," might explain some parameters."),(0,r.kt)("h4",{id:"clickhouse-extras"},"ClickHouse extras"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"parameter")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default value")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hadoop","_","kerberos","_","keytab"),(0,r.kt)("td",{parentName:"tr",align:null},'""')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hadoop","_","kerberos","_","principal"),(0,r.kt)("td",{parentName:"tr",align:null},'""')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"libhdfs3","_","conf"),(0,r.kt)("td",{parentName:"tr",align:null},'""')))),(0,r.kt)("h3",{id:"limitations"},"Limitations"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"hadoop_security_kerberos_ticket_cache_path")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"libhdfs3_conf")," can be global only, not user specific")),(0,r.kt)("h2",{id:"kerberos-support"},"Kerberos support"),(0,r.kt)("p",null,"If the ",(0,r.kt)("inlineCode",{parentName:"p"},"hadoop_security_authentication")," parameter has the value ",(0,r.kt)("inlineCode",{parentName:"p"},"kerberos"),", ClickHouse authenticates via Kerberos.\nParameters are ",(0,r.kt)("a",{parentName:"p",href:"#clickhouse-extras"},"here")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"hadoop_security_kerberos_ticket_cache_path")," may be of help.\nNote that due to libhdfs3 limitations only old-fashioned approach is supported,\ndatanode communications are not secured by SASL (",(0,r.kt)("inlineCode",{parentName:"p"},"HADOOP_SECURE_DN_USER")," is a reliable indicator of such\nsecurity approach). Use ",(0,r.kt)("inlineCode",{parentName:"p"},"tests/integration/test_storage_kerberized_hdfs/hdfs_configs/bootstrap.sh")," for reference."),(0,r.kt)("p",null,"If ",(0,r.kt)("inlineCode",{parentName:"p"},"hadoop_kerberos_keytab"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"hadoop_kerberos_principal")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"hadoop_security_kerberos_ticket_cache_path")," are specified, Kerberos authentication will be used. ",(0,r.kt)("inlineCode",{parentName:"p"},"hadoop_kerberos_keytab")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"hadoop_kerberos_principal")," are mandatory in this case."),(0,r.kt)("h2",{id:"namenode-ha"},"HDFS Namenode HA support"),(0,r.kt)("p",null,"libhdfs3 support HDFS namenode HA."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Copy ",(0,r.kt)("inlineCode",{parentName:"li"},"hdfs-site.xml")," from an HDFS node to ",(0,r.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/"),"."),(0,r.kt)("li",{parentName:"ul"},"Add following piece to ClickHouse config file:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"  <hdfs>\n    <libhdfs3_conf>/etc/clickhouse-server/hdfs-site.xml</libhdfs3_conf>\n  </hdfs>\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Then use ",(0,r.kt)("inlineCode",{parentName:"li"},"dfs.nameservices")," tag value of ",(0,r.kt)("inlineCode",{parentName:"li"},"hdfs-site.xml")," as the namenode address in the HDFS URI. For example, replace ",(0,r.kt)("inlineCode",{parentName:"li"},"hdfs://appadmin@192.168.101.11:8020/abc/")," with ",(0,r.kt)("inlineCode",{parentName:"li"},"hdfs://appadmin@my_nameservice/abc/"),".")),(0,r.kt)("h2",{id:"virtual-columns"},"Virtual Columns"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"_path")," \u2014 Path to the file."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"_file")," \u2014 Name of the file.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"See Also")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/AlgoliaDocsSelfCrawl/en/engines/table-engines/#table_engines-virtual_columns"},"Virtual columns"))))}m.isMDXComponent=!0}}]);