"use strict";(self.webpackChunkclickhouse_docs_2_3_0=self.webpackChunkclickhouse_docs_2_3_0||[]).push([[87049],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>k});var i=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),c=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},h=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(n),h=a,k=d["".concat(l,".").concat(h)]||d[h]||u[h]||o;return n?i.createElement(k,r(r({ref:t},p),{},{components:n})):i.createElement(k,r({ref:t},p))}));function k(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,r=new Array(o);r[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:a,r[1]=s;for(var c=2;c<o;c++)r[c]=n[c];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}h.displayName="MDXCreateElement"},51948:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var i=n(87462),a=(n(67294),n(3905));const o={slug:"/en/guides/sre/s3-multi-region",sidebar_label:"Replicating a single shard across two AWS regions using S3 Object Storage"},r="Replicating a single shard across two AWS regions using S3 Object Storage",s={unversionedId:"en/integrations/data-ingestion/s3/s3-multi-region",id:"en/integrations/data-ingestion/s3/s3-multi-region",title:"Replicating a single shard across two AWS regions using S3 Object Storage",description:"Object storage is used by default in ClickHouse Cloud, you do not need to follow this procedure if you are running in ClickHouse Cloud.",source:"@site/docs/en/integrations/data-ingestion/s3/s3-multi-region.md",sourceDirName:"en/integrations/data-ingestion/s3",slug:"/en/guides/sre/s3-multi-region",permalink:"/docs/en/guides/sre/s3-multi-region",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/integrations/data-ingestion/s3/s3-multi-region.md",tags:[],version:"current",frontMatter:{slug:"/en/guides/sre/s3-multi-region",sidebar_label:"Replicating a single shard across two AWS regions using S3 Object Storage"},sidebar:"english",previous:{title:"Replicating a single shard across two GCP regions using Google Cloud Storage (GCS)",permalink:"/docs/en/guides/sre/gcs-multi-region"},next:{title:"Use an ETL Tool",permalink:"/docs/en/integrations/data-ingestion/etl-tools"}},l={},c=[{value:"Plan the deployment",id:"plan-the-deployment",level:2},{value:"Install software",id:"install-software",level:2},{value:"ClickHouse server nodes",id:"clickhouse-server-nodes",level:3},{value:"Deploy ClickHouse",id:"deploy-clickhouse",level:3},{value:"Deploy ClickHouse Keeper",id:"deploy-clickhouse-keeper",level:3},{value:"Create S3 Buckets",id:"create-s3-buckets",level:2},{value:"Configure ClickHouse Keeper",id:"configure-clickhouse-keeper",level:2},{value:"Configure ClickHouse Server",id:"configure-clickhouse-server",level:2},{value:"Define a cluster",id:"define-a-cluster",level:3},{value:"Disable zero-copy replication",id:"disable-zero-copy-replication",level:3},{value:"Configure networking",id:"configure-networking",level:2},{value:"Start the servers",id:"start-the-servers",level:2},{value:"Run ClickHouse Keeper",id:"run-clickhouse-keeper",level:3},{value:"Check ClickHouse Keeper status",id:"check-clickhouse-keeper-status",level:4},{value:"Run ClickHouse Server",id:"run-clickhouse-server",level:3},{value:"Verify ClickHouse Server",id:"verify-clickhouse-server",level:4},{value:"Testing",id:"testing",level:2}],p={toc:c},d="wrapper";function u(e){let{components:t,...o}=e;return(0,a.kt)(d,(0,i.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"replicating-a-single-shard-across-two-aws-regions-using-s3-object-storage"},"Replicating a single shard across two AWS regions using S3 Object Storage"),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"Object storage is used by default in ClickHouse Cloud, you do not need to follow this procedure if you are running in ClickHouse Cloud.")),(0,a.kt)("h2",{id:"plan-the-deployment"},"Plan the deployment"),(0,a.kt)("p",null,"This tutorial is based on deploying two ClickHouse Server nodes and three ClickHouse Keeper nodes in AWS EC2.  The data store for the ClickHouse servers is S3. Two AWS regions, with a ClickHouse Server and an S3 Bucket in each region, are used in order to support disaster recovery."),(0,a.kt)("p",null,"ClickHouse tables are replicated across the two servers, and therefore across the two regions."),(0,a.kt)("h2",{id:"install-software"},"Install software"),(0,a.kt)("h3",{id:"clickhouse-server-nodes"},"ClickHouse server nodes"),(0,a.kt)("p",null,"Refer to the ",(0,a.kt)("a",{parentName:"p",href:"/docs/en/install/#available-installation-options"},"installation instructions")," when performing the deployment steps on the ClickHouse server nodes and ClickHouse Keeper nodes."),(0,a.kt)("h3",{id:"deploy-clickhouse"},"Deploy ClickHouse"),(0,a.kt)("p",null,"Deploy ClickHouse on two hosts, in the sample configurations these are named ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode1"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode2"),"."),(0,a.kt)("p",null,"Place ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode1")," in one AWS region, and ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode2")," in a second."),(0,a.kt)("h3",{id:"deploy-clickhouse-keeper"},"Deploy ClickHouse Keeper"),(0,a.kt)("p",null,"Deploy ClickHouse Keeper on three hosts, in the sample configurations these are named ",(0,a.kt)("inlineCode",{parentName:"p"},"keepernode1"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"keepernode2"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"keepernode3"),".  ",(0,a.kt)("inlineCode",{parentName:"p"},"keepernode1")," can be deployed in the same region as ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode1"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"keepernode2")," with ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode2"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"keepernode3")," in either region but a different availability zone from the ClickHouse node in that region."),(0,a.kt)("admonition",{type:"note"},(0,a.kt)("p",{parentName:"admonition"},"ClickHouse Keeper is installed the same way as ClickHouse, as it can be run with ClickHouse server, or standalone.  Running Keeper standalone gives more flexibility when scaling out or upgrading.")),(0,a.kt)("p",null,"Once you deploy ClickHouse on the three Keeper nodes run these commands to prep the directories for configuration and operation in standalone mode:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"sudo mkdir /etc/clickhouse-keeper\nsudo chown clickhouse:clickhouse /etc/clickhouse-keeper\nsudo chmod 700 /etc/clickhouse-keeper\nsudo mkdir -p /var/lib/clickhouse/coordination\nsudo chown -R clickhouse:clickhouse /var/lib/clickhouse\n")),(0,a.kt)("h2",{id:"create-s3-buckets"},"Create S3 Buckets"),(0,a.kt)("p",null,"Creating S3 buckets is covered in the guide ",(0,a.kt)("a",{parentName:"p",href:"/docs/en/guides/sre/configuring-s3-for-clickhouse-use"},"use S3 Object Storage as a ClickHouse disk"),". Create two S3 buckets, one in each of the regions that you have placed ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode1")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode2"),".  The configuration files will then be placed in ",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-server/config.d/"),".  Here is a sample configuration file for one bucket, the other is similar with the three highlighted lines differing:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/storage_config.xml"',title:'"/etc/clickhouse-server/config.d/storage_config.xml"'},"<clickhouse>\n  <storage_configuration>\n     <disks>\n        <s3_disk>\n           <type>s3</type>\n    \x3c!--highlight-start--\x3e\n           <endpoint>https://docs-clickhouse-s3.s3.us-east-2.amazonaws.com/clickhouses3/</endpoint>\n           <access_key_id>ABCDEFGHIJKLMNOPQRST</access_key_id>\n           <secret_access_key>Tjdm4kf5snfkj303nfljnev79wkjn2l3knr81007</secret_access_key>\n    \x3c!--highlight-end--\x3e\n           <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>\n        </s3_disk>\n     \n        <s3_cache>\n           <type>cache</type>\n           <disk>s3</disk>\n           <path>/var/lib/clickhouse/disks/s3_cache/</path>\n           <max_size>10Gi</max_size>\n        </s3_cache>\n     </disks>\n        <policies>\n            <s3_main>\n                <volumes>\n                    <main>\n                        <disk>s3_disk</disk>\n                    </main>\n                </volumes>\n            </s3_main>\n    </policies>\n   </storage_configuration>\n</clickhouse>\n")),(0,a.kt)("admonition",{type:"note"},(0,a.kt)("p",{parentName:"admonition"},"Many of the steps in this guide will ask you to place a configuration file in ",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-server/config.d/"),".  This is the default location on Linux systems for configuration override files.  When you put these files into that directory ClickHouse will use the content to override the default configuration.  By placing these files in the override directory you will avoid losing your configuration during an upgrade.")),(0,a.kt)("h2",{id:"configure-clickhouse-keeper"},"Configure ClickHouse Keeper"),(0,a.kt)("p",null,"When running ClickHouse Keeper standalone (separate from ClickHouse server) the configuration is a single XML file.  In this tutorial, the file is ",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-keeper/keeper.xml"),".  All three Keeper servers use the same configuration with one setting different; ",(0,a.kt)("inlineCode",{parentName:"p"},"<server_id>"),"."),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"server_id")," indicates the ID to be assigned to the host where the configuration files is used.  In the example below, the ",(0,a.kt)("inlineCode",{parentName:"p"},"server_id")," is ",(0,a.kt)("inlineCode",{parentName:"p"},"3"),", and if you look further down in the file in the ",(0,a.kt)("inlineCode",{parentName:"p"},"<raft_configuration>")," section, you will see that server 3 has the hostname ",(0,a.kt)("inlineCode",{parentName:"p"},"keepernode3"),".  This is how the ClickHouse Keeper process knows which other servers to connect to when choosing a leader and all other activities."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-keeper/keeper.xml"',title:'"/etc/clickhouse-keeper/keeper.xml"'},"<clickhouse>\n    <listen_host>0.0.0.0</listen_host>\n\n    <keeper_server>\n        <tcp_port>9181</tcp_port>\n    \x3c!--highlight-start--\x3e\n        <server_id>3</server_id>\n    \x3c!--highlight-end--\x3e\n        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n        <coordination_settings>\n            <operation_timeout_ms>10000</operation_timeout_ms>\n            <session_timeout_ms>30000</session_timeout_ms>\n            <raft_logs_level>warning</raft_logs_level>\n        </coordination_settings>\n\n        <raft_configuration>\n            <server>\n                <id>1</id>\n                <hostname>keepernode1</hostname>\n                <port>9234</port>\n            </server>\n            <server>\n                <id>2</id>\n                <hostname>keepernode2</hostname>\n                <port>9234</port>\n            </server>\n    \x3c!--highlight-start--\x3e\n            <server>\n                <id>3</id>\n                <hostname>keepernode3</hostname>\n                <port>9234</port>\n            </server>\n        \x3c!--highlight-end--\x3e\n        </raft_configuration>\n    </keeper_server>\n</clickhouse>\n")),(0,a.kt)("p",null,"Copy the configuration file for ClickHouse Keeper in place (remembering to set the ",(0,a.kt)("inlineCode",{parentName:"p"},"<server_id>"),"):"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"sudo -u clickhouse \\\n  cp keeper.xml /etc/clickhouse-keeper/keeper.xml\n")),(0,a.kt)("h2",{id:"configure-clickhouse-server"},"Configure ClickHouse Server"),(0,a.kt)("h3",{id:"define-a-cluster"},"Define a cluster"),(0,a.kt)("p",null,"ClickHouse cluster(s) are defined in the ",(0,a.kt)("inlineCode",{parentName:"p"},"<remote_servers>")," section of the configuration.  In this sample one cluster, ",(0,a.kt)("inlineCode",{parentName:"p"},"cluster_1S_2R"),", is defined and it consists of a single shard with two replicas.  The replicas are located on the hosts ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode1")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode2"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/remote-servers.xml"',title:'"/etc/clickhouse-server/config.d/remote-servers.xml"'},'<clickhouse>\n    <remote_servers replace="true">\n        <cluster_1S_2R>\n            <shard>\n                <replica>\n                    <host>chnode1</host>\n                    <port>9000</port>\n                </replica>\n                <replica>\n                    <host>chnode2</host>\n                    <port>9000</port>\n                </replica>\n            </shard>\n        </cluster_1S_2R>\n    </remote_servers>\n</clickhouse>\n')),(0,a.kt)("p",null,"When working with clusters it is handy to define macros that populate DDL queries with the cluster, shard, and replica settings.  This sample allows you to specify the use of a replicated table engine without providing ",(0,a.kt)("inlineCode",{parentName:"p"},"shard")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"replica")," details.  When you create a table you can see how the ",(0,a.kt)("inlineCode",{parentName:"p"},"shard")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"replica")," macros are used by querying ",(0,a.kt)("inlineCode",{parentName:"p"},"system.tables"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/macros.xml"',title:'"/etc/clickhouse-server/config.d/macros.xml"'},"<clickhouse>\n    <distributed_ddl>\n            <path>/clickhouse/task_queue/ddl</path>\n    </distributed_ddl>\n    <macros>\n        <cluster>cluster_1S_2R</cluster>\n        <shard>1</shard>\n        <replica>replica_1</replica>\n    </macros>\n</clickhouse>\n")),(0,a.kt)("admonition",{type:"note"},(0,a.kt)("p",{parentName:"admonition"},"The above macros are for ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode1"),", on ",(0,a.kt)("inlineCode",{parentName:"p"},"chnode2")," set ",(0,a.kt)("inlineCode",{parentName:"p"},"replica")," to ",(0,a.kt)("inlineCode",{parentName:"p"},"replica_2"),".")),(0,a.kt)("h3",{id:"disable-zero-copy-replication"},"Disable zero-copy replication"),(0,a.kt)("p",null,"In ClickHouse versions 22.7 and lower the setting ",(0,a.kt)("inlineCode",{parentName:"p"},"allow_remote_fs_zero_copy_replication")," is set to ",(0,a.kt)("inlineCode",{parentName:"p"},"true")," by default for S3 and HDFS disks. This setting should be set to ",(0,a.kt)("inlineCode",{parentName:"p"},"false")," for this disaster recovery scenario, and in version 22.8 and higher it is set to ",(0,a.kt)("inlineCode",{parentName:"p"},"false")," by default."),(0,a.kt)("p",null,"This setting should be false for two reasons: 1) this feature is not production ready; 2) in a disaster recovery scenario both the data and metadata need to be stored in multiple regions. Set ",(0,a.kt)("inlineCode",{parentName:"p"},"allow_remote_fs_zero_copy_replication")," to ",(0,a.kt)("inlineCode",{parentName:"p"},"false"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/remote-servers.xml"',title:'"/etc/clickhouse-server/config.d/remote-servers.xml"'},"<clickhouse>\n   <merge_tree>\n        <allow_remote_fs_zero_copy_replication>false</allow_remote_fs_zero_copy_replication>\n   </merge_tree>\n</clickhouse>\n")),(0,a.kt)("p",null,"ClickHouse Keeper is responsible for coordinating the replication of data across the ClickHouse nodes.  To inform ClickHouse about the ClickHouse Keeper nodes add a configuration file to each of the ClickHouse nodes."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/use_keeper.xml"',title:'"/etc/clickhouse-server/config.d/use_keeper.xml"'},'<clickhouse>\n    <zookeeper>\n        <node index="1">\n            <host>keepernode1</host>\n            <port>9181</port>\n        </node>\n        <node index="2">\n            <host>keepernode2</host>\n            <port>9181</port>\n        </node>\n        <node index="3">\n            <host>keepernode3</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n</clickhouse>\n')),(0,a.kt)("h2",{id:"configure-networking"},"Configure networking"),(0,a.kt)("p",null,"See the ",(0,a.kt)("a",{parentName:"p",href:"./network-ports"},"network ports")," list when you configure the security settings in AWS so that your servers can communicate with each other, and you can communicate with them."),(0,a.kt)("p",null,"All three servers must listen for network connections so that they can communicate between the servers and with S3.  By default, ClickHouse listens ony on the loopback address, so this must be changed.  This is configured in ",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-server/config.d/"),".  Here is a sample that configures ClickHouse and ClickHouse Keeper to listen on all IP v4 interfaces.  see the documentation or the default configuration file ",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse/config.xml")," for more information."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/networking.xml"',title:'"/etc/clickhouse-server/config.d/networking.xml"'},"<clickhouse>\n    <listen_host>0.0.0.0</listen_host>\n</clickhouse>\n")),(0,a.kt)("h2",{id:"start-the-servers"},"Start the servers"),(0,a.kt)("h3",{id:"run-clickhouse-keeper"},"Run ClickHouse Keeper"),(0,a.kt)("p",null,"On each Keeper server:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"sudo -u clickhouse \\\n  clickhouse-keeper -C /etc/clickhouse-keeper/keeper.xml --daemon\n")),(0,a.kt)("h4",{id:"check-clickhouse-keeper-status"},"Check ClickHouse Keeper status"),(0,a.kt)("p",null,"Send commands to the ClickHouse Keeper with ",(0,a.kt)("inlineCode",{parentName:"p"},"netcat"),".  For example, ",(0,a.kt)("inlineCode",{parentName:"p"},"mntr")," returns the state of the ClickHouse Keeper cluster.  If you run the command on each of the Keeper nodes you will see that one is a leader, and the other two are followers:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"echo mntr | nc localhost 9181\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-response"},"zk_version  v22.7.2.15-stable-f843089624e8dd3ff7927b8a125cf3a7a769c069\nzk_avg_latency  0\nzk_max_latency  11\nzk_min_latency  0\nzk_packets_received 1783\nzk_packets_sent 1783\n# highlight-start\nzk_num_alive_connections    2\nzk_outstanding_requests 0\nzk_server_state leader\n# highlight-end\nzk_znode_count  135\nzk_watch_count  8\nzk_ephemerals_count 3\nzk_approximate_data_size    42533\nzk_key_arena_size   28672\nzk_latest_snapshot_size 0\nzk_open_file_descriptor_count   182\nzk_max_file_descriptor_count    18446744073709551615\n# highlight-start\nzk_followers    2\nzk_synced_followers 2\n# highlight-end\n")),(0,a.kt)("h3",{id:"run-clickhouse-server"},"Run ClickHouse Server"),(0,a.kt)("p",null,"On each ClickHouse server run"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"sudo service clickhouse-server start\n")),(0,a.kt)("h4",{id:"verify-clickhouse-server"},"Verify ClickHouse Server"),(0,a.kt)("p",null,"When you added the ",(0,a.kt)("a",{parentName:"p",href:"#define-a-cluster"},"cluster configuration")," a single shard replicated across the two ClickHouse nodes was defined.  In this verification step you will check that the cluster was built when ClickHouse was started, and you will create a replicated table using that cluster."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Verify that the cluster exists:"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"show clusters\n")),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500cluster\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 cluster_1S_2R \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 0.009 sec. `\n"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Create a table in the cluster using the ",(0,a.kt)("inlineCode",{parentName:"p"},"ReplicatedMergeTree")," table engine:"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"create table trips on cluster 'cluster_1S_2R' (\n `trip_id` UInt32,\n `pickup_date` Date,\n `pickup_datetime` DateTime,\n `dropoff_datetime` DateTime,\n `pickup_longitude` Float64,\n `pickup_latitude` Float64,\n `dropoff_longitude` Float64,\n `dropoff_latitude` Float64,\n `passenger_count` UInt8,\n `trip_distance` Float64,\n `tip_amount` Float32,\n `total_amount` Float32,\n `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4))\nENGINE = ReplicatedMergeTree\nPARTITION BY toYYYYMM(pickup_date)\nORDER BY pickup_datetime\nSETTINGS index_granularity = 8192, storage_policy='s3_main'\n")),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode1 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode2 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Understand the use of the macros defined earlier"),(0,a.kt)("p",{parentName:"li"},"The macros ",(0,a.kt)("inlineCode",{parentName:"p"},"shard"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"replica")," were ",(0,a.kt)("a",{parentName:"p",href:"#define-a-cluster"},"defined earlier"),", and in the highlighted line below you can see where the values are substituted on each ClickHouse node.  Additionally, the value ",(0,a.kt)("inlineCode",{parentName:"p"},"uuid")," is used; ",(0,a.kt)("inlineCode",{parentName:"p"},"uuid")," is not defined in the macros as it is generated by the system."),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT create_table_query\nFROM system.tables\nWHERE name = 'trips'\nFORMAT Vertical\n")),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 4d326b66-0402-4c14-9c2f-212bedd282c0\n\nRow 1:\n\u2500\u2500\u2500\u2500\u2500\u2500\ncreate_table_query: CREATE TABLE default.trips (`trip_id` UInt32, `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_datetime` DateTime, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `tip_amount` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4))\n# highlight-next-line\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{uuid}/{shard}', '{replica}')\nPARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime SETTINGS index_granularity = 8192, storage_policy = 's3_main'\n\n1 row in set. Elapsed: 0.012 sec.\n")),(0,a.kt)("admonition",{parentName:"li",type:"note"},(0,a.kt)("p",{parentName:"admonition"},"You can customize the zookeeper path ",(0,a.kt)("inlineCode",{parentName:"p"},"'clickhouse/tables/{uuid}/{shard}")," shown above by setting ",(0,a.kt)("inlineCode",{parentName:"p"},"default_replica_path")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"default_replica_name"),".  The docs are ",(0,a.kt)("a",{parentName:"p",href:"/docs/en/operations/server-configuration-parameters/settings/#default_replica_path"},"here"),".")))),(0,a.kt)("h2",{id:"testing"},"Testing"),(0,a.kt)("p",null,"These tests will verify that data is being replicated across the two servers, and that it is stored in the S3 Buckets and not on local disk."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Add data from the New York City taxi dataset:"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO trips\nSELECT trip_id,\n       pickup_date,\n       pickup_datetime,\n       dropoff_datetime,\n       pickup_longitude,\n       pickup_latitude,\n       dropoff_longitude,\n       dropoff_latitude,\n       passenger_count,\n       trip_distance,\n       tip_amount,\n       total_amount,\n       payment_type\n   FROM s3('https://ch-nyc-taxi.s3.eu-west-3.amazonaws.com/tsv/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames') LIMIT 1000000;\n"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Verify that data is stored in S3."),(0,a.kt)("p",{parentName:"li"},"This query shows the size of the data on disk, and the policy used to determine which disk is used."),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT\n    engine,\n    data_paths,\n    metadata_path,\n    storage_policy,\n    formatReadableSize(total_bytes)\nFROM system.tables\nWHERE name = 'trips'\nFORMAT Vertical\n")),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-response"},"Query id: af7a3d1b-7730-49e0-9314-cc51c4cf053c\n\nRow 1:\n\u2500\u2500\u2500\u2500\u2500\u2500\nengine:                          ReplicatedMergeTree\ndata_paths:                      ['/var/lib/clickhouse/disks/s3_disk/store/551/551a859d-ec2d-4512-9554-3a4e60782853/']\nmetadata_path:                   /var/lib/clickhouse/store/e18/e18d3538-4c43-43d9-b083-4d8e0f390cf7/trips.sql\nstorage_policy:                  s3_main\nformatReadableSize(total_bytes): 36.42 MiB\n\n1 row in set. Elapsed: 0.009 sec.\n")),(0,a.kt)("p",{parentName:"li"},"Check the size of data on the local disk.  From above, the size on disk for the millions of rows stored is 36.42 MiB.  This should be on S3, and not the local disk.  The query above also tells us where on local disk data and metadata is stored.  Check the local data:"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-response"},"root@chnode1:~# du -sh /var/lib/clickhouse/disks/s3_disk/store/551\n536K    /var/lib/clickhouse/disks/s3_disk/store/551\n")),(0,a.kt)("p",{parentName:"li"},"Check the S3 data in each S3 Bucket (the totals are not shown, but both buckets have approximately 36 MiB stored after the inserts):"),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"size in first S3 bucket",src:n(46862).Z,width:"1315",height:"935"})),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"size in second S3 bucket",src:n(24731).Z,width:"1315",height:"935"})))))}u.isMDXComponent=!0},46862:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/bucket1-68884eb470ad9b974ecbe9ea36c9cdfa.png"},24731:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/bucket2-4a39a7d29034721f8bb52dbb6faad814.png"}}]);