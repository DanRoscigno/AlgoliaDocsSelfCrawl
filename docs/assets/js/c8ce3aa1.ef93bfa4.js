"use strict";(self.webpackChunkclickhouse_docs_2_3_0=self.webpackChunkclickhouse_docs_2_3_0||[]).push([[80931],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>k});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=c(n),h=i,k=d["".concat(l,".").concat(h)]||d[h]||p[h]||o;return n?a.createElement(k,r(r({ref:t},u),{},{components:n})):a.createElement(k,r({ref:t},u))}));function k(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:i,r[1]=s;for(var c=2;c<o;c++)r[c]=n[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},96164:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=n(87462),i=(n(67294),n(3905));const o={slug:"/en/guides/sre/gcs-multi-region",sidebar_label:"Replicating a single shard across two GCP regions using Google Cloud Storage (GCS)",title:"Replicating a single shard across two GCP regions using Google Cloud Storage (GCS)"},r="Using Google Cloud Storage (GCS)",s={unversionedId:"en/integrations/data-ingestion/s3/gcs-multi-region",id:"en/integrations/data-ingestion/s3/gcs-multi-region",title:"Replicating a single shard across two GCP regions using Google Cloud Storage (GCS)",description:"Object storage is used by default in ClickHouse Cloud, you do not need to follow this procedure if you are running in ClickHouse Cloud.",source:"@site/docs/en/integrations/data-ingestion/s3/gcs-multi-region.md",sourceDirName:"en/integrations/data-ingestion/s3",slug:"/en/guides/sre/gcs-multi-region",permalink:"/docs/en/guides/sre/gcs-multi-region",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/integrations/data-ingestion/s3/gcs-multi-region.md",tags:[],version:"current",frontMatter:{slug:"/en/guides/sre/gcs-multi-region",sidebar_label:"Replicating a single shard across two GCP regions using Google Cloud Storage (GCS)",title:"Replicating a single shard across two GCP regions using Google Cloud Storage (GCS)"},sidebar:"english",previous:{title:"Use S3 Object Storage as a ClickHouse disk",permalink:"/docs/en/guides/sre/configuring-s3-for-clickhouse-use"},next:{title:"Replicating a single shard across two AWS regions using S3 Object Storage",permalink:"/docs/en/guides/sre/s3-multi-region"}},l={},c=[{value:"Plan the deployment",id:"plan-the-deployment",level:2},{value:"Prepare VMs",id:"prepare-vms",level:2},{value:"Deploy ClickHouse",id:"deploy-clickhouse",level:3},{value:"Deploy ClickHouse Keeper",id:"deploy-clickhouse-keeper",level:3},{value:"Create two buckets",id:"create-two-buckets",level:2},{value:"ch_bucket_us_east1",id:"ch_bucket_us_east1",level:3},{value:"ch_bucket_us_east4",id:"ch_bucket_us_east4",level:3},{value:"Generate an Access key",id:"generate-an-access-key",level:2},{value:"Create a service account HMAC key and secret",id:"create-a-service-account-hmac-key-and-secret",level:3},{value:"Add a new service account",id:"add-a-new-service-account",level:3},{value:"Configure ClickHouse Keeper",id:"configure-clickhouse-keeper",level:2},{value:"Configure ClickHouse Server",id:"configure-clickhouse-server",level:2},{value:"Networking",id:"networking",level:3},{value:"Remote ClickHouse Keeper servers",id:"remote-clickhouse-keeper-servers",level:3},{value:"Remote ClickHouse servers",id:"remote-clickhouse-servers",level:3},{value:"Replica identification",id:"replica-identification",level:3},{value:"Storage in GCS",id:"storage-in-gcs",level:3},{value:"Start ClickHouse Keeper",id:"start-clickhouse-keeper",level:2},{value:"Check ClickHouse Keeper status",id:"check-clickhouse-keeper-status",level:3},{value:"Start ClickHouse server",id:"start-clickhouse-server",level:2},{value:"Verification",id:"verification",level:2},{value:"Verify disk configuration",id:"verify-disk-configuration",level:3},{value:"Verify that tables created on the cluster are created on both nodes",id:"verify-that-tables-created-on-the-cluster-are-created-on-both-nodes",level:3},{value:"Verify that data can be inserted",id:"verify-that-data-can-be-inserted",level:3},{value:"Verify that the storage policy <code>gcs_main</code> is used for the table.",id:"verify-that-the-storage-policy-gcs_main-is-used-for-the-table",level:3},{value:"Verify in Google Cloud Console",id:"verify-in-google-cloud-console",level:3},{value:"Bucket for replica one",id:"bucket-for-replica-one",level:4},{value:"Bucket for replica two",id:"bucket-for-replica-two",level:4}],u={toc:c},d="wrapper";function p(e){let{components:t,...o}=e;return(0,i.kt)(d,(0,a.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"using-google-cloud-storage-gcs"},"Using Google Cloud Storage (GCS)"),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"Object storage is used by default in ClickHouse Cloud, you do not need to follow this procedure if you are running in ClickHouse Cloud.")),(0,i.kt)("h2",{id:"plan-the-deployment"},"Plan the deployment"),(0,i.kt)("p",null,'This tutorial is written to describe a replicated ClickHouse deployment running in Google Cloud and using Google Cloud Storage (GCS) as the ClickHouse storage disk "type".'),(0,i.kt)("p",null,"In the tutorial, you will deploy ClickHouse server nodes in Google Cloud Engine VMs, each with an associated GCS bucket for storage.  Replication is coordinated by a set of ClickHouse Keeper nodes, also deployed as VMs."),(0,i.kt)("p",null,"Sample requirements for high availability:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Two ClickHouse server nodes, in two GCP regions"),(0,i.kt)("li",{parentName:"ul"},"Two GCS buckets, deployed in the same regions as the two ClickHouse server nodes"),(0,i.kt)("li",{parentName:"ul"},"Three ClickHouse Keeper nodes, two of them are deployed in the same regions as the ClickHouse server nodes. The third can be in the same region as one of the first two Keeper nodes, but in a different availability zone.")),(0,i.kt)("p",null,"ClickHouse Keeper requires two nodes to function, hence a requirement for three nodes for high availability."),(0,i.kt)("h2",{id:"prepare-vms"},"Prepare VMs"),(0,i.kt)("p",null,"Deploy five VMS in three regions:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Region"),(0,i.kt)("th",{parentName:"tr",align:null},"ClickHouse Server"),(0,i.kt)("th",{parentName:"tr",align:null},"Bucket"),(0,i.kt)("th",{parentName:"tr",align:null},"ClickHouse Keeper"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"1"),(0,i.kt)("td",{parentName:"tr",align:null},"chnode1"),(0,i.kt)("td",{parentName:"tr",align:null},"bucket_regionname"),(0,i.kt)("td",{parentName:"tr",align:null},"keepernode1")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"2"),(0,i.kt)("td",{parentName:"tr",align:null},"chnode2"),(0,i.kt)("td",{parentName:"tr",align:null},"bucket_regionname"),(0,i.kt)("td",{parentName:"tr",align:null},"keepernode2")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"3 ",(0,i.kt)("inlineCode",{parentName:"td"},"*")),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"keepernode3")))),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"*")," This can be a different availability zone in the same region as 1 or 2."),(0,i.kt)("h3",{id:"deploy-clickhouse"},"Deploy ClickHouse"),(0,i.kt)("p",null,"Deploy ClickHouse on two hosts, in the sample configurations these are named ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2"),"."),(0,i.kt)("p",null,"Place ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1")," in one GCP region, and ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2")," in a second.  In this guide ",(0,i.kt)("inlineCode",{parentName:"p"},"us-east1")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"us-east4")," are used for the compute engine VMs, and also for GCS buckets."),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Do not start ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse server")," until after it is configured.  Just install it.")),(0,i.kt)("p",null,"Refer to the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#available-installation-options"},"installation instructions")," when performing the deployment steps on the ClickHouse server nodes and ClickHouse Keeper nodes."),(0,i.kt)("h3",{id:"deploy-clickhouse-keeper"},"Deploy ClickHouse Keeper"),(0,i.kt)("p",null,"Deploy ClickHouse Keeper on three hosts, in the sample configurations these are named ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode2"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode3"),".  ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode1")," can be deployed in the same region as ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode2")," with ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode3")," in either region, but in a different availability zone from the ClickHouse node in that region."),(0,i.kt)("p",null,"Refer to the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#available-installation-options"},"installation instructions")," when performing the deployment steps on the ClickHouse server nodes and ClickHouse Keeper nodes."),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"ClickHouse Keeper is installed in the same way as ClickHouse, as it can be run with ClickHouse server, or standalone.  Running Keeper standalone gives more flexibility when scaling out or upgrading.")),(0,i.kt)("p",null,"Once you deploy ClickHouse on the three Keeper nodes run these commands to prep the directories for configuration and operation in standalone mode:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo mkdir /etc/clickhouse-keeper\nsudo chown clickhouse:clickhouse /etc/clickhouse-keeper\nsudo chmod 700 /etc/clickhouse-keeper\nsudo mkdir -p /var/lib/clickhouse/coordination\nsudo chown -R clickhouse:clickhouse /var/lib/clickhouse\n")),(0,i.kt)("h2",{id:"create-two-buckets"},"Create two buckets"),(0,i.kt)("p",null,"The two ClickHouse servers will be located in different regions for high availability.  Each will have a GCS bucket in the same region."),(0,i.kt)("p",null,"In ",(0,i.kt)("strong",{parentName:"p"},"Cloud Storage > Buckets")," choose ",(0,i.kt)("strong",{parentName:"p"},"CREATE BUCKET"),". For this tutorial two buckets are created, one in each of ",(0,i.kt)("inlineCode",{parentName:"p"},"us-east1")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"us-east4"),".  The buckets are single region, standard storage class, and not public.  When prompted, enable public access prevention.  Do not create folders, they will be created when ClickHouse writes to the storage."),(0,i.kt)("h3",{id:"ch_bucket_us_east1"},"ch_bucket_us_east1"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(32741).Z,width:"1437",height:"387"})),(0,i.kt)("h3",{id:"ch_bucket_us_east4"},"ch_bucket_us_east4"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(14045).Z,width:"1437",height:"386"})),(0,i.kt)("h2",{id:"generate-an-access-key"},"Generate an Access key"),(0,i.kt)("h3",{id:"create-a-service-account-hmac-key-and-secret"},"Create a service account HMAC key and secret"),(0,i.kt)("p",null,"Open ",(0,i.kt)("strong",{parentName:"p"},"Cloud Storage > Settings > Interoperability")," and either choose an existing ",(0,i.kt)("strong",{parentName:"p"},"Access key"),", or ",(0,i.kt)("strong",{parentName:"p"},"CREATE A KEY FOR A SERVICE ACCOUNT"),".  This guide covers the path for creating a new key for a new service account."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(68642).Z,width:"969",height:"911"})),(0,i.kt)("h3",{id:"add-a-new-service-account"},"Add a new service account"),(0,i.kt)("p",null,"If this is a project with no existing service account, ",(0,i.kt)("strong",{parentName:"p"},"CREATE NEW ACCOUNT"),"."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(87911).Z,width:"924",height:"317"})),(0,i.kt)("p",null,"There are three steps to creating the service account, in the first step give the account a meaningful name, ID, and description."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(34377).Z,width:"842",height:"737"})),(0,i.kt)("p",null,"In the Interoperability settings dialog the IAM role ",(0,i.kt)("strong",{parentName:"p"},"Storage Object Admin")," role is recommended; select that role in step two."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(9458).Z,width:"822",height:"396"})),(0,i.kt)("p",null,"Step three is optional and not used in this guide.  You may allow users to have these privileges based on your policies."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(39137).Z,width:"635",height:"697"})),(0,i.kt)("p",null,"The service account HMAC key will be displayed.  Save this information, as it will be used in the ClickHouse configuration."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Add a bucket",src:n(92979).Z,width:"917",height:"390"})),(0,i.kt)("h2",{id:"configure-clickhouse-keeper"},"Configure ClickHouse Keeper"),(0,i.kt)("p",null,"All of the ClickHouse Keeper nodes have the same configuration file except for the ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," line (first highlighted line below).  Modify the file with the hostnames for your ClickHouse Keeper servers, and on each of the servers set the ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," to match the appropriate ",(0,i.kt)("inlineCode",{parentName:"p"},"server")," entry in the ",(0,i.kt)("inlineCode",{parentName:"p"},"raft_configuration"),".  Since this example has ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," set to ",(0,i.kt)("inlineCode",{parentName:"p"},"3"),", we have highlighted the matching lines in the ",(0,i.kt)("inlineCode",{parentName:"p"},"raft_configuration"),"."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Edit the file with your hostnames, and make sure that they resolve from the ClickHouse server nodes and the Keeper nodes"),(0,i.kt)("li",{parentName:"ul"},"Copy the file into place (",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-keeper/keeper-config.xml")," on each of the Keeper servers"),(0,i.kt)("li",{parentName:"ul"},"Edit the ",(0,i.kt)("inlineCode",{parentName:"li"},"server_id")," on each machine, based on its entry number in the ",(0,i.kt)("inlineCode",{parentName:"li"},"raft_configuration"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:"title=/etc/clickhouse-keeper/keeper-config.xml",title:"/etc/clickhouse-keeper/keeper-config.xml"},"<clickhouse>\n    <listen_host>0.0.0.0</listen_host>\n\n    <keeper_server>\n        <tcp_port>9181</tcp_port>\n\x3c!--highlight-next-line--\x3e\n        <server_id>3</server_id>\n        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n        <coordination_settings>\n            <operation_timeout_ms>10000</operation_timeout_ms>\n            <session_timeout_ms>30000</session_timeout_ms>\n            <raft_logs_level>warning</raft_logs_level>\n        </coordination_settings>\n\n        <raft_configuration>\n            <server>\n                <id>1</id>\n                <hostname>keepernode1.us-east1-b.c.clickhousegcs-374921.internal</hostname>\n                <port>9234</port>\n            </server>\n            <server>\n                <id>2</id>\n                <hostname>keepernode2.us-east4-c.c.clickhousegcs-374921.internal</hostname>\n                <port>9234</port>\n            </server>\n\x3c!--highlight-start--\x3e\n            <server>\n                <id>3</id>\n                <hostname>keepernode3.us-east5-a.c.clickhousegcs-374921.internal</hostname>\n                <port>9234</port>\n            </server>\n\x3c!--highlight-end--\x3e\n        </raft_configuration>\n    </keeper_server>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"configure-clickhouse-server"},"Configure ClickHouse Server"),(0,i.kt)("admonition",{title:"best practice",type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Some of the steps in this guide will ask you to place a configuration file in ",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-server/config.d/"),".  This is the default location on Linux systems for configuration override files.  When you put these files into that directory ClickHouse will merge the content with the default configuration.  By placing these files in the ",(0,i.kt)("inlineCode",{parentName:"p"},"config.d")," directory you will avoid losing your configuration during an upgrade.")),(0,i.kt)("h3",{id:"networking"},"Networking"),(0,i.kt)("p",null,"By default, ClickHouse listens on the loopback interface, in a replicated setup networking between machines is necessary.  Listen on all interfaces:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:"title=/etc/clickhouse-server/config.d/network.xml",title:"/etc/clickhouse-server/config.d/network.xml"},"<clickhouse>\n    <listen_host>0.0.0.0</listen_host>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"remote-clickhouse-keeper-servers"},"Remote ClickHouse Keeper servers"),(0,i.kt)("p",null,"Replication is coordinated by ClickHouse Keeper.  This configuration file identifies the ClickHouse Keeper nodes by hostname and port number."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Edit the hostnames to match your Keeper hosts")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:"title=/etc/clickhouse-server/config.d/use-keeper.xml",title:"/etc/clickhouse-server/config.d/use-keeper.xml"},'<clickhouse>\n    <zookeeper>\n        <node index="1">\n            <host>keepernode1.us-east1-b.c.clickhousegcs-374921.internal</host>\n            <port>9181</port>\n        </node>\n        <node index="2">\n            <host>keepernode2.us-east4-c.c.clickhousegcs-374921.internal</host>\n            <port>9181</port>\n        </node>\n        <node index="3">\n            <host>keepernode3.us-east5-a.c.clickhousegcs-374921.internal</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n</clickhouse>\n')),(0,i.kt)("h3",{id:"remote-clickhouse-servers"},"Remote ClickHouse servers"),(0,i.kt)("p",null,"This file configures the hostname and port of each ClickHouse server in the cluster.  The default configuration file contains sample cluster definitions, in order to show only the clusters that are completely configured the tag ",(0,i.kt)("inlineCode",{parentName:"p"},'replace="true"')," is added to the ",(0,i.kt)("inlineCode",{parentName:"p"},"remote_servers")," entry so that when this configuration is merged with the default it replaces the ",(0,i.kt)("inlineCode",{parentName:"p"},"remote_servers")," section instead of adding to it."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Edit the file with your hostnames, and make sure that they resolve from the ClickHouse server nodes")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:"title=/etc/clickhouse-server/config.d/remote-servers.xml",title:"/etc/clickhouse-server/config.d/remote-servers.xml"},'<clickhouse>\n    <remote_servers replace="true">\n        <cluster_1S_2R>\n            <shard>\n                <replica>\n                    <host>chnode1.us-east1-b.c.clickhousegcs-374921.internal</host>\n                    <port>9000</port>\n                </replica>\n                <replica>\n                    <host>chnode2.us-east4-c.c.clickhousegcs-374921.internal</host>\n                    <port>9000</port>\n                </replica>\n            </shard>\n        </cluster_1S_2R>\n    </remote_servers>\n</clickhouse>\n')),(0,i.kt)("h3",{id:"replica-identification"},"Replica identification"),(0,i.kt)("p",null,"This file configures settings related to the ClickHouse Keeper path.  Specifically the macros used to identify which replica the data is part of.  On one server the replica should be specified as ",(0,i.kt)("inlineCode",{parentName:"p"},"replica_1"),", and on the other server ",(0,i.kt)("inlineCode",{parentName:"p"},"replica_2"),".  The names can be changed, based on our example of one replica being stored in South Carolina and the other in Northern Virginia the values could be ",(0,i.kt)("inlineCode",{parentName:"p"},"carolina")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"virginia"),"; just make sure that they are different on each machine."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:"title=/etc/clickhouse-server/config.d/macros.xml",title:"/etc/clickhouse-server/config.d/macros.xml"},"<clickhouse>\n    <distributed_ddl>\n            <path>/clickhouse/task_queue/ddl</path>\n    </distributed_ddl>\n    <macros>\n        <cluster>cluster_1S_2R</cluster>\n        <shard>1</shard>\n\x3c!--highlight-next-line--\x3e\n        <replica>replica_1</replica>\n    </macros>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"storage-in-gcs"},"Storage in GCS"),(0,i.kt)("p",null,"ClickHouse storage configuration includes ",(0,i.kt)("inlineCode",{parentName:"p"},"disks")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"policies"),". The disk being configured below is named ",(0,i.kt)("inlineCode",{parentName:"p"},"gcs"),", and is of ",(0,i.kt)("inlineCode",{parentName:"p"},"type")," ",(0,i.kt)("inlineCode",{parentName:"p"},"s3"),".  The type is s3 because ClickHouse accesses the GCS bucket as if it was an AWS S3 bucket.  Two copies of this configuration will be needed, one for each of the ClickHouse server nodes."),(0,i.kt)("p",null,"These substitutions should be made in the configuration below."),(0,i.kt)("p",null,"These substitutions differ between the two ClickHouse server nodes:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"REPLICA 1 BUCKET")," should be set to the name of the bucket in the same region as the server"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"REPLICA 1 FOLDER")," should be changed to ",(0,i.kt)("inlineCode",{parentName:"li"},"replica_1")," on one of the servers, and ",(0,i.kt)("inlineCode",{parentName:"li"},"replica_2")," on the other")),(0,i.kt)("p",null,"These substitutions are common across the two nodes:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The ",(0,i.kt)("inlineCode",{parentName:"li"},"access_key_id")," should be set to the HMAC Key generated earlier"),(0,i.kt)("li",{parentName:"ul"},"The ",(0,i.kt)("inlineCode",{parentName:"li"},"secret_access_key")," should be set to HMAC Secret generated earlier")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:"title=/etc/clickhouse-server/config.d/storage.xml",title:"/etc/clickhouse-server/config.d/storage.xml"},"<clickhouse>\n    <storage_configuration>\n        <disks>\n            <gcs>\n                <support_batch_delete>false</support_batch_delete>\n                <type>s3</type>\n                <endpoint>https://storage.googleapis.com/REPLICA 1 BUCKET/REPLICA 1 FOLDER/</endpoint>\n                <access_key_id>SERVICE ACCOUNT HMAC KEY</access_key_id>\n                <secret_access_key>SERVICE ACCOUNT HMAC SECRET</secret_access_key>\n                <metadata_path>/var/lib/clickhouse/disks/gcs/</metadata_path>\n            </gcs>\n        <cache>\n                <type>cache</type>\n                <disk>gcs</disk>\n                <path>/var/lib/clickhouse/disks/gcs_cache/</path>\n                <max_size>10Gi</max_size>\n            </cache>\n        </disks>\n        <policies>\n            <gcs_main>\n                <volumes>\n                    <main>\n                        <disk>gcs</disk>\n                    </main>\n                </volumes>\n            </gcs_main>\n        </policies>\n    </storage_configuration>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"start-clickhouse-keeper"},"Start ClickHouse Keeper"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo -u clickhouse clickhouse-keeper --config-file=/etc/clickhouse-keeper/keeper-config.xml --daemon\n")),(0,i.kt)("h3",{id:"check-clickhouse-keeper-status"},"Check ClickHouse Keeper status"),(0,i.kt)("p",null,"Send commands to the ClickHouse Keeper with ",(0,i.kt)("inlineCode",{parentName:"p"},"netcat"),".  For example, ",(0,i.kt)("inlineCode",{parentName:"p"},"mntr")," returns the state of the ClickHouse Keeper cluster.  If you run the command on each of the Keeper nodes you will see that one is a leader, and the other two are followers:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"echo mntr | nc localhost 9181\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"zk_version  v22.7.2.15-stable-f843089624e8dd3ff7927b8a125cf3a7a769c069\nzk_avg_latency  0\nzk_max_latency  11\nzk_min_latency  0\nzk_packets_received 1783\nzk_packets_sent 1783\n# highlight-start\nzk_num_alive_connections    2\nzk_outstanding_requests 0\nzk_server_state leader\n# highlight-end\nzk_znode_count  135\nzk_watch_count  8\nzk_ephemerals_count 3\nzk_approximate_data_size    42533\nzk_key_arena_size   28672\nzk_latest_snapshot_size 0\nzk_open_file_descriptor_count   182\nzk_max_file_descriptor_count    18446744073709551615\n# highlight-start\nzk_followers    2\nzk_synced_followers 2\n# highlight-end\n")),(0,i.kt)("h2",{id:"start-clickhouse-server"},"Start ClickHouse server"),(0,i.kt)("p",null,"On ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode")," run:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo service clickhouse-server start\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo service clickhouse-server status\n")),(0,i.kt)("h2",{id:"verification"},"Verification"),(0,i.kt)("h3",{id:"verify-disk-configuration"},"Verify disk configuration"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"system.disks")," should contain records for each disk:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"default"),(0,i.kt)("li",{parentName:"ul"},"gcs"),(0,i.kt)("li",{parentName:"ul"},"cache")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT *\nFROM system.disks\nFORMAT Vertical\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"Row 1:\n\u2500\u2500\u2500\u2500\u2500\u2500\nname:             cache\npath:             /var/lib/clickhouse/disks/gcs/\nfree_space:       18446744073709551615\ntotal_space:      18446744073709551615\nunreserved_space: 18446744073709551615\nkeep_free_space:  0\ntype:             s3\nis_encrypted:     0\nis_read_only:     0\nis_write_once:    0\nis_remote:        1\nis_broken:        0\ncache_path:       /var/lib/clickhouse/disks/gcs_cache/\n\nRow 2:\n\u2500\u2500\u2500\u2500\u2500\u2500\nname:             default\npath:             /var/lib/clickhouse/\nfree_space:       6555529216\ntotal_space:      10331889664\nunreserved_space: 6555529216\nkeep_free_space:  0\ntype:             local\nis_encrypted:     0\nis_read_only:     0\nis_write_once:    0\nis_remote:        0\nis_broken:        0\ncache_path:       \n\nRow 3:\n\u2500\u2500\u2500\u2500\u2500\u2500\nname:             gcs\npath:             /var/lib/clickhouse/disks/gcs/\nfree_space:       18446744073709551615\ntotal_space:      18446744073709551615\nunreserved_space: 18446744073709551615\nkeep_free_space:  0\ntype:             s3\nis_encrypted:     0\nis_read_only:     0\nis_write_once:    0\nis_remote:        1\nis_broken:        0\ncache_path:       \n\n3 rows in set. Elapsed: 0.002 sec. \n")),(0,i.kt)("h3",{id:"verify-that-tables-created-on-the-cluster-are-created-on-both-nodes"},"Verify that tables created on the cluster are created on both nodes"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"# highlight-next-line\ncreate table trips on cluster 'cluster_1S_2R' (\n `trip_id` UInt32,\n `pickup_date` Date,\n `pickup_datetime` DateTime,\n `dropoff_datetime` DateTime,\n `pickup_longitude` Float64,\n `pickup_latitude` Float64,\n `dropoff_longitude` Float64,\n `dropoff_latitude` Float64,\n `passenger_count` UInt8,\n `trip_distance` Float64,\n `tip_amount` Float32,\n `total_amount` Float32,\n `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4))\nENGINE = ReplicatedMergeTree\nPARTITION BY toYYYYMM(pickup_date)\nORDER BY pickup_datetime\n# highlight-next-line\nSETTINGS storage_policy='gcs_main'\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode2.us-east4-c.c.gcsqa-375100.internal \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode1.us-east1-b.c.gcsqa-375100.internal \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2 rows in set. Elapsed: 0.641 sec. \n")),(0,i.kt)("h3",{id:"verify-that-data-can-be-inserted"},"Verify that data can be inserted"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO trips SELECT\n    trip_id,\n    pickup_date,\n    pickup_datetime,\n    dropoff_datetime,\n    pickup_longitude,\n    pickup_latitude,\n    dropoff_longitude,\n    dropoff_latitude,\n    passenger_count,\n    trip_distance,\n    tip_amount,\n    total_amount,\n    payment_type\nFROM s3('https://ch-nyc-taxi.s3.eu-west-3.amazonaws.com/tsv/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames')\nLIMIT 1000000\n")),(0,i.kt)("h3",{id:"verify-that-the-storage-policy-gcs_main-is-used-for-the-table"},"Verify that the storage policy ",(0,i.kt)("inlineCode",{parentName:"h3"},"gcs_main")," is used for the table."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT\n    engine,\n    data_paths,\n    metadata_path,\n    storage_policy,\n    formatReadableSize(total_bytes)\nFROM system.tables\nWHERE name = 'trips'\nFORMAT Vertical\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"Row 1:\n\u2500\u2500\u2500\u2500\u2500\u2500\nengine:                          ReplicatedMergeTree\ndata_paths:                      ['/var/lib/clickhouse/disks/gcs/store/631/6315b109-d639-4214-a1e7-afbd98f39727/']\nmetadata_path:                   /var/lib/clickhouse/store/e0f/e0f3e248-7996-44d4-853e-0384e153b740/trips.sql\nstorage_policy:                  gcs_main\nformatReadableSize(total_bytes): 36.42 MiB\n\n1 row in set. Elapsed: 0.002 sec. \n")),(0,i.kt)("h3",{id:"verify-in-google-cloud-console"},"Verify in Google Cloud Console"),(0,i.kt)("p",null,"Looking at the buckets you will see that a folder was created in each bucket with the name that was used in the ",(0,i.kt)("inlineCode",{parentName:"p"},"storage.xml")," configuration file.  Expand the folders and you will see many files, representing the data partitions."),(0,i.kt)("h4",{id:"bucket-for-replica-one"},"Bucket for replica one"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"replica one bucket",src:n(70853).Z,width:"958",height:"736"})),(0,i.kt)("h4",{id:"bucket-for-replica-two"},"Bucket for replica two"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"replica two bucket",src:n(50349).Z,width:"958",height:"736"})))}p.isMDXComponent=!0},32741:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-bucket-1-cb78309853226fcbc256301b346a2841.png"},14045:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-bucket-2-435e325de9e8f6a1106060046c1b0ffb.png"},68642:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-create-a-service-account-key-340ac67e07ac1bd496dc68e064c68edd.png"},87911:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-create-service-account-0-53fadb51c6af2f89942b1e285a6eba2b.png"},9458:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-create-service-account-2-07cd3badb327d57014a274f0b1d74bc4.png"},39137:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-create-service-account-3-dd4bfbb2ca8e38546d19a6b8f723a028.png"},34377:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-create-service-account-a-f82eb9fa02349e0d912c83484f59faaf.png"},70853:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-examine-bucket-1-62d8f4fccb4232ec3afeeea62e2ae56a.png"},50349:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-examine-bucket-2-5a9b7bd02764737577d52ffd0437f1ac.png"},92979:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GCS-guide-key-f8d11583aa24c03fcad9dc7aac080489.png"}}]);